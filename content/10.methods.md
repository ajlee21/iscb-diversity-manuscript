## Materials and Methods

### Honoree Curation

#### ISCB Fellows Recipients

We examined the ISCB [webpage of ISCB Fellows](http://web.archive.org/web/20200116150052/https://www.iscb.org/iscb-fellows).
We found recipients listed for the years 2009-2019.
We gleaned the full name of the fellow as well as the year in which they received the honor.
We used the name as provided on the site.
For certain methods we were required to split the full name into first and last names.
In this case we chose the first non-initial name as the first name and the final name as the last name.
We did not consider a hyphen to be a name separator:
for hyphenated names, all components were included.

#### ISMB Keynote Speakers

We examined the webpage for each ISMB meeting.
We were able to successfully find pages with keynote speakers for the years 2002-2019.
We gleaned the full name of each keynote speaker as well as the year in which they delivered a keynote.
We used the name as provided on the site.
We split names into first and last names as described for ISCB Fellows.

#### PSB Keynote Speakers

We examined the webpage for each PSB meeting.
We were able to successfully find pages with keynote speakers for the years 1999-2020.
We gleaned the full name of each keynote speaker as well as the year in which they delivered a keynote.
We used the name as provided on the site.
We split names into first and last names as described for ISCB Fellows.

#### RECOMB Keynote Speakers

We examined the webpage for each RECOMB meeting.
We found conference webpages with keynote speakers for 1999, 2000, 2001, 2004, 2007, 2008, and 2010-2019.
We were able to fill in the missing years using information from the RECOMB 2016 proceedings, which summarizes the first 20 years of the RECOMB conference [@doi:10.1007/978-3-319-31957-5].
This volume has two tables of keynote speakers from 1997-2006 (Table 14, page XXVII) and 2007-2016 (Table 4, page 8).
Using these tables to verify the conference speaker lists, we arrived at two special instances of inclusion/exclusion.
Although Jun Wang was not included in these tables, we were able to confirm that he was a keynote speaker in 2011 with the RECOMB 2011 proceedings [@doi:10.1007/978-3-642-20036-6], and thus we include this speaker in the dataset.
Marian Walhout was invited as a keynote speaker but had to [cancel](http://recomb2015.mimuw.edu.pl/node/18.html) the talk due to other obligations.
Because her name was neither mentioned in the 2015 proceedings [@doi:10.1007/978-3-319-16706-0] nor in the earlier tables, we exclude this speaker from our dataset.
For other keynote speakers, we gleaned their full name as well as the year in which they delivered a keynote.
We used the name as provided on the site.
We split names into first and last names as described for ISCB Fellows.

### Corresponding author extraction

We assumed that research advisors in the field would be those most likely to be invited for keynotes or to be honored as Fellows.
Therefore, we collected corresponding author names to assess the composition of the field, weighted by the number of publications.

We evaluated two resources for extracting corresponding authors from papers: [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/) (PMC).
Both resources are provided by the U.S. National Library of Medicine and index scholarly articles.
PubMed contains a record for every article published in journals it indexes (30 million records total circa 2020) and provides abstracts but not fulltext.
PMC, which provides fulltext access, does not contain every article from every journal (5.9 million records total circa 2020).
In general, open access journals will deposit their entire catalog to PMC (e.g. _BMC Bioinformatics_ & _PLOS Computational Biology_), while toll access journals (e.g. _Bioinformatics_) will only deposit articles when funders require it.
Since PMC requires publishers to submit fulltext articles in a structured XML format, the machine-readability and breadth of metadata in PMC is often superior to PubMed.

Of PMC's 5.9 million fulltext articles, only 2.7 million are part of the "[Open Access Subset](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/)" which allows for downloading the structured fulltext as opposed to just viewing the article online.
However, authorship information does not require full text records.
We were able to download structured frontmatter (rather than fulltext) records from PMC's [OAI-PMH service](https://www.ncbi.nlm.nih.gov/pmc/tools/oai/), so we were not limited to just the Open Access Subset.
For PubMed, we used the E-Utilities APIs.
For PubMed records, we were able to extract author names (first and last) and order.
For PMC, we were able to extract these fields as well as whether each author was a corresponding author.
To automate and generalize these tasks, we created the [pubmedpy](https://github.com/dhimmel/pubmedpy) Python package.

We selected three journals to represent the field of bioinformatics and computational biology, including two ISCB Partner Journals (_PLOS Computational Biology_ and _Bioinformatics_) and one field-specific journal that is not a partner (_BMC Bioinformatics_).
From PubMed, we compiled a catalog of 29,755 journal articles published from when each journal was established through 2019.
We were able to retrieve authorship information for all but 6 of these articles using PubMed or PubMed Central.

To determine corresponding authors for an article, we relied on PMC data if available (20,696 articles) and otherwise fellback to PubMed data (9,053 articles).
Almost all articles without PMC data were from _Bioinformatics_, since its a "selective deposit" rather than "full paricipation" journal in PMC.

We performed further analysis on PMC authors to learn more about corresponding author practices.
First, we developed and evaluated a method to infer a corresponding author when the coded corresponding status was not available.
For papers with multiple authors and at least one corresponding author, the first author was corresponding 43% of the time, whearas the last author was corresponding 62% of the time.
Therefore, we assumed the last author was corresponding when coded corresponding author status was not available (120 articles from PMC and all articles from PubMed).

Second, we investigated the number of corresponding authors for PMC articles.
81% of these articles had a single corresponding author.
1.7% had no corresponding authors.
Of these, many were editorials (e.g. [PMC1183510](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1183510/), the announcement of _PLOS Computational Biology_).
A very small number of papers had over 10 corresponding authors.
Some of these instances were true outliers, like [PMC5001208](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5001208/) with 21 corresponding authors.
Others like [PMC3509495](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3509495/) were incorrect, due to upstream errors.
To not give undue influence to papers with multiple corresponding authors, subsequent analyses on corresponding authors are weighted by 1 divided by the number of corresponding authors per paper.

### Estimation of Gender

We predicted the gender of honorees and authors using the <https://genderize.io> API that produces predictions trained on over 100 million name-gender pairings collected from the web.
We used author and honoree first names to retrieve predictions from genderize.io.
The predictions, which consider gender as a binary trait, represent the probability of an honoree or author being male or female.

### Estimation of Race and Ethnicity

We predicted the race and ethnicity of honorees and authors using the R package `wru`.
`wru` implements methods described in Imai and Khanna [@doi:10.1093/pan/mpw001] to predict race and ethnicity using surname and location information.
The underlying data used for prediction are derived from the United States Census.
We used only the surname of author or honoree to make predictions via the `predict_race()` function.
However, in the case of names that were not observed in the census, the function's behavior was to use the average demographic distribution from the census.
We modified the function to return a status denoting that results were inconclusive instead.
This prediction represents the probability of an honoree or author selecting a certain race or ethnicity on a census form if they lived within the United States.

### Estimation of Nationality

To complement wru's race and ethnicity estimation, we applied the Python package `ethnicolr` developed by Sood and Laohaprapanon [@arxiv:1805.02109].
`ethnicolr` also uses United States Census data, but also adds in data curated from Wikipedia in 2009 [@doi:10.1145/1557019.1557032].
While ethnicolr overcomes some of the limitations of wru, its international representation is still limited.
For instance, 76% of the names in ethnicolr’s Wikipedia dataset are European in origin, and the dataset contains remarkably fewer Asian, African, and Middle Eastern names compared to that of wru. 

A number of the limitations of ethnicolr are related to the source data.
Since Wikipedia has grown substantially since 2009, we created an updated dataset and set of predictors based on the ethnicolr methodology, which are described below.

#### Constructing a Name-to-Nationality Dataset

To generate a training dataset for nationality prediction, we scraped Wikipedia’s category of [Living People](https://en.wikipedia.org/wiki/Category:Living_people) (approximately 930,000 pages).
This category is regularly curated and allowed us to avoid pages related to non-persons and to reflect a modern naming landscape.
For each Wikipedia page, we used two strategies to find a full birth name and nationality for that person.
First, we pulled information from the personal details sidebar; the information in this sidebar varied widely but usually contained a full name and a place of birth.
Second, in the body of the text of most English-language biographical Wikipedia pages, the first sentence usually begins with, for example, “John Edward Smith (born 1 January 1970) is an American novelist known for ...”
We used regular expressions to parse out the person’s name from this structure, as well as checking that the expression after “is a” matched a list of possible nationalities.
Using a union of these strategies, we were able to define a name and nationality for 708,493 people via their Wikipedia pages.
Our Wikipedia-based process returned only a nationality or country of origin, which was more fine-grained than the broader regional patterns that we sought to examine among ISCB honorees and bioinformatics authors.
We initially annotated based on continent, but after several iterations settled on the following categories: Hispanic (including Latin America and Iberia), African, Israeli, Muslim, SouthAsian, EastAsian, European (non-British, non-Iberian), and CelticEnglish (including United States, Canada, and Australia).
We make this dataset, which we term Wiki2019, available at <https://github.com/greenelab/wiki-nationality-estimate/master/data/annotated_names.tsv>.

#### Nationality Prediction with LSTM Neural Networks
We trained a Long Short-term Memory (LSTM) neural network, the same kind of classifier used by ethnicolr.
This classifier learned to recognize patterns in the sequences of letters in a name to infer region of origin.
We evaluated multiple lengths of letters and, based on this comparison, used tri-characters for the primary results described in this work.
To evaluate our classifier, we used unseen examples from ethnicolr’s Wiki2009 dataset as ground truth to test our best model (LSTM, 3-grams).
Our prediction model, which we term Wiki2019-LSTM is available at <https://github.com/greenelab/wiki-nationality-estimate/master/models/LSTM.h5>.
